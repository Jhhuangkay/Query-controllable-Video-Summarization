{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define transforms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Define the transform.\n",
    "train_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),             # takes PIL image as input and outputs PIL image\n",
    "        transforms.ToTensor(),              # takes PIL image as input and outputs torch.tensor\n",
    "        transforms.Normalize(mean=[0.4280, 0.4106, 0.3589],  # takes tensor and outputs tensor\n",
    "                             std=[0.2737, 0.2631, 0.2601]),  # see next step for mean and std\n",
    "    ])\n",
    "\n",
    "valid_transform = transforms.Compose([ \n",
    "        transforms.Resize((224,224)),             \n",
    "#         transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4280, 0.4106, 0.3589],\n",
    "                             std=[0.2737, 0.2631, 0.2601]), \n",
    "    ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),             \n",
    "#         transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4280, 0.4106, 0.3589],\n",
    "                             std=[0.2737, 0.2631, 0.2601]), \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the dataset for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "\n",
    "class dataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the frames.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.query_frame_train = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.query_frame_train) \n",
    "\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                    self.query_frame_train.iloc[idx, 2].split(\"/\")[len(self.query_frame_train.iloc[idx, 2].split(\"/\"))-2],\n",
    "                    self.query_frame_train.iloc[idx, 2].split(\"/\")[len(self.query_frame_train.iloc[idx, 2].split(\"/\"))-1])\n",
    "        image = io.imread(img_name) \n",
    "        query = self.query_frame_train.iloc[idx, 0]\n",
    "        score_annotations = self.query_frame_train.iloc[idx, 3:] \n",
    "        score_annotations = np.array([score_annotations])\n",
    "\n",
    "        score_annotations = score_annotations.astype('float').reshape(-1, )\n",
    "        \n",
    "        sample = {'image': image, 'query': query, 'score_annotations': score_annotations}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(Image.fromarray(sample['image']))\n",
    "            sample['score_annotations'] = torch.from_numpy(sample['score_annotations'])\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_embedding(box_train_query, word2index_x, max_length):\n",
    "    one_hot_x_list = encode_queries_index(box_train_query, word2index_x)\n",
    "    one_hot_x_tensor = []\n",
    "    for i in one_hot_x_list:\n",
    "        one_hot_x_tensor.append(torch.FloatTensor(i))\n",
    "\n",
    "    one_hot_x_tensor_padded = pad_sequence(one_hot_x_tensor, batch_first=True, padding_value=0)\n",
    "    \n",
    "    one_hot_x_tensor_padded_with_same_max_length = []\n",
    "    for i in one_hot_x_tensor_padded:\n",
    "        if len(i) < max_length:\n",
    "            i = torch.cat((i, torch.zeros(max_length -len(i))), dim=0)\n",
    "        else:\n",
    "            i = i[:8]\n",
    "            \n",
    "        one_hot_x_tensor_padded_with_same_max_length.append(i)\n",
    "        \n",
    "    return torch.stack(one_hot_x_tensor_padded_with_same_max_length) # return a stack of tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BasicModule import BasicModule\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet34\n",
    "\n",
    "class QVSmodel(BasicModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(QVSmodel, self).__init__()\n",
    "        \n",
    "        self.model = resnet34(pretrained='imagenet')\n",
    "        self.model = models.resnet34(pretrained=True) \n",
    "        self.fc1 = torch.nn.Linear(512, 2)\n",
    "        \n",
    "        self.fc_text = torch.nn.Linear(8, 512)\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)     \n",
    "    \n",
    "    \n",
    "        x = F.avg_pool2d(x, 7)\n",
    "        \n",
    "        # reshape x\n",
    "        x = x.view(x.size(0), -1)\n",
    "    \n",
    "        y = F.relu(self.fc_text(y))\n",
    "        \n",
    "        #Combine x and y by element-wise multiplication. The output dimension is still (1, 512).\n",
    "        t1 = torch.mul(x, y)\n",
    "\n",
    "        #Computes the second fully connected layer\n",
    "        relevance_class_prediction = self.fc1(t1)\n",
    "        \n",
    "        return relevance_class_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "def trainNet(model, batch_size, n_epochs, learning_rate):\n",
    "    \n",
    "    # For GPU\n",
    "    net = model.cuda()\n",
    "    net.train()  \n",
    "    \n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"epochs=\", n_epochs)\n",
    "    print(\"learning_rate=\", learning_rate)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    #Get data\n",
    "    data_loader = dataset\n",
    "    n_batches = len(data_loader) \n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr = learning_rate)\n",
    "    \n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    log_file = open('PATH') \n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        count_relevance = 0\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()  \n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i_batch, sample_batched in enumerate(data_loader):\n",
    "            \n",
    "            labels_relevance = labels[:, 0]\n",
    "            \n",
    "            inputs = inputs.cuda()\n",
    "            labels_relevance = labels_relevance.cuda()\n",
    "    \n",
    "            #Get inputs\n",
    "            inputs, query, labels = sample_batched['image'], sample_batched['query'], \\\n",
    "            sample_batched['score_annotations']          \n",
    "            \n",
    "            #Wrap them in a Variable object\n",
    "            inputs, labels_relevance = Variable(inputs), Variable(labels_relevance)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "\n",
    "            loss_size_1 = loss(outputs[0], labels_relevance.long())\n",
    "            loss_size = loss_size_1\n",
    "            loss_size.backward()\n",
    "            optimizer.step()  \n",
    "   \n",
    "            running_loss += loss_size.item()\n",
    "    \n",
    "            total_train_loss += loss_size.item()  \n",
    "           \n",
    "            #Compute accuracy\n",
    "            max_values_relevance, arg_maxs_relevance = torch.max(outputs[0], dim = 1)\n",
    "            num_correct_relevance = torch.sum(labels_relevance.long() == arg_maxs_relevance.long())\n",
    "            count_relevance = count_relevance + num_correct_relevance.item()\n",
    "        \n",
    "        \n",
    "            print(\"Epoch {}, {:d}% \\t train_loss_{}_batch: {:.4f} \\t took: {:.4f}s\".format(\n",
    "                        epoch+1, int(100 * (i_batch+1) / len(data_loader)), i_batch+1, running_loss, time.time() - start_time))\n",
    "\n",
    "            log_file.write(\"Epoch {}, {:d}% \\t train_loss_{}_batch: {:.4f} \\t took: {:.4f}s \\n\".format(\n",
    "                        epoch+1, int(100 * (i_batch+1) / len(data_loader)), i_batch+1, running_loss, time.time() - start_time))\n",
    "            log_file.flush()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            start_time = time.time()\n",
    "\n",
    "        acc_relevance = (float(count_relevance)/(len(train_loader)*199))\n",
    "        print(\"Training accuracy_relevance = {:.4f} for epoch {}\".format(acc_relevance, epoch +1))\n",
    "\n",
    "\n",
    "    print(\"Training finished, took {:.4f}s\".format(time.time() - training_start_time))\n",
    "    \n",
    "    log_file.close() \n",
    "    return net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
